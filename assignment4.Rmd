---
title: "Assignment 4"
author: "Group1"
date: "24/02/2022"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
library(tidyverse)
library(ggplot2)  
library(here)
library(janitor) # clean_names()
library(skimr)
library(vroom)
library(mosaic)
library(lubridate)
library(readxl)
library(quantmod)
library(xts)
library(moments)
library(readxl)
library(forecast)
library(roll)
library(fGarch)
library(qcc)
library(MTS)
library(caret)
library(TTR)
library(devtools)
library(ggfortify)
```

# Question 1 - VaR
```{r}
#Sheet 1
hpr_daily <- read_excel("PS1_Daily.xlsx", sheet=1, skip = 1) %>% 
  mutate(DATE = as.Date(DATE))
```

```{r, log returns}
# pick msft, ge and jpm and calculating log returns 
hpr_daily <- hpr_daily %>%
  mutate(
    Ret_msft = log(1 + MSFT),
    Ret_ge = log(1 + GE),
    Ret_jpm = log(1 + JPM),
    Ret_sprtrn = log(1 + SPRTRN)
  )

```

## Calculating EWMA for each stock
```{r}
# calculate 10-week MA for each stock
msft_vol10w <- sqrt(runMean((hpr_daily$Ret_msft) ^ 2, 50))
ge_vol10w <- sqrt(runMean((hpr_daily$Ret_ge) ^ 2, 50))
jpm_vol10w <- sqrt(runMean((hpr_daily$Ret_jpm) ^ 2, 50))


# we can also use another method 
# vol<-TTR::SMA((hpr_daily$Ret_msft)^2, n = 50, fill = NA)

# define EWMA model function
ewma.func <- function(rets, lambda, sig0) {
	sig.p <- sig0
	sig.s <-
	  vapply(rets, function(r)
	    sig.p <<- sig.p * lambda + (r ^ 2) * (1 - lambda), 0)
	return(sqrt(sig.s))
}

# obtain sigma0 for each stock
msft.s0 <- mean((hpr_daily$Ret_msft) ^ 2)
ge.s0 <- mean((hpr_daily$Ret_ge) ^ 2)
jpm.s0 <- mean((hpr_daily$Ret_jpm) ^ 2)

# calculate EWMA for each stock
msft_ewma <- ewma.func(hpr_daily$Ret_msft, 0.94, msft.s0)
ge_ewma <- ewma.func(hpr_daily$Ret_ge, 0.94, ge.s0)
jpm_ewma <- ewma.func(hpr_daily$Ret_jpm, 0.94, jpm.s0)

```

```{r}
# cumulative mean return time series
msft.mu <- cummean(hpr_daily$Ret_msft)
ge.mu <- cummean(hpr_daily$Ret_ge)
jpm.mu <- cummean(hpr_daily$Ret_jpm)

# calculate VaR for each stock using EWMA volatility
df <- cbind(hpr_daily, msft.mu, msft_ewma, ge.mu, ge_ewma, jpm.mu, jpm_ewma)
df <- df %>% 
  mutate(msft.VaR = msft.mu - 1.65 * msft_ewma,
         ge.VaR = ge.mu - 1.65 * ge_ewma,
         jpm.VaR = jpm.mu - 1.65 * jpm_ewma)

```

## Backtesting VaR
```{r}
# count times of violation
# count the negative realized market returns that are more extreme than the VaR on this given day.
df<- df %>% 
  mutate(jpm.violation = case_when( Ret_jpm<0 & Ret_jpm < jpm.VaR ~ TRUE ,TRUE ~ FALSE),
         msft.violation = case_when(Ret_msft<0& Ret_msft< msft.VaR ~ TRUE ,TRUE ~ FALSE),
         ge.violation = case_when( Ret_ge<0 & Ret_ge< ge.VaR ~ TRUE,TRUE ~ FALSE)) 

cat("Violations of JPM: ", sum(df$jpm.violation), 
    "\nViolations of Microsoft: ", sum(df$msft.violation),
    "\nViolations of GE: ", sum(df$ge.violation))


```

*Answer 1:*
Value a Risk tells us the maximum loss in a given time period at a specific level of significance/probability. Further, while there are certain caveats to using this such as setting an appropriate time frame (time period for VaR must be long enough for corrective measures and needs to reflect the liquidity of assets) and using significant amount of data for calculations. VaR is compulsorily used by lots of companies (as enforced by the regulatory bodies) - required capital against market risk is measured by VaR (Basel I) and required capital against credit and operational risk is also measured by VaR (Basel II).

Back-testing measures the accuracy of the value at risk calculations i.e. checking when the trading losses are greater than the predicted VaR. The less violations there are, they better are our VaR estimates. Over the period  of 6301 days, there are violations around 4% of these days which suggests that our VaR model didn't capture the 4% extreme loss. However, we can say that over a time period of 6301 days, the loss will not exceed the VaR value 96% of the time. 

# Question 2 - Loading the data frames
```{r}
#Data  for portfolios on operating profit 
op<- read.csv("Portfolios_Formed_on_OP.CSV", skip = 24, stringsAsFactors = FALSE, strip.white=TRUE, nrow=702) %>%
  clean_names()

op_clean <- op %>%
  rename(date=x)

op_clean$date <- parse_date(as.character(op_clean$date), format = "%Y%m")

head(op_clean)

#Data on portfoios formed on investmnet 
inv <- read.csv("Portfolios_Formed_on_INV.CSV", skip = 17, stringsAsFactors = FALSE, strip.white=TRUE,  nrow=702) %>%
  clean_names()

inv_clean <- inv %>%
  rename(date=x)

inv_clean$date <- parse_date(as.character(inv_clean$date), format = "%Y%m")

head(inv_clean)

#Data on portfolios formed on portfolios by dividend yeild 
dy <- read.csv("Portfolios_Formed_on_D-P.CSV", skip = 19, stringsAsFactors = FALSE, strip.white=TRUE, nrow=1134) %>%
  clean_names()

dy_clean <- dy %>%
  rename(date=x)

dy_clean$date <- parse_date(as.character(dy_clean$date), format = "%Y%m")

head(dy_clean)

#Data on momentum 
mom <- read.csv("10_Portfolios_Prior_12_2.CSV", skip = 10, stringsAsFactors = FALSE, strip.white=TRUE, nrows = 1140) %>% 
  clean_names()

mom_clean <- mom %>%
  rename(date=x)

mom_clean$date <- parse_date(as.character(mom_clean$date), format = "%Y%m")

head(mom_clean)

#49 portfolios data 
port49 <- read.csv("49_Industry_Portfolios.CSV",skip = 11, stringsAsFactors = FALSE, strip.white=TRUE, nrows =1146) %>% 
  clean_names()

port49_clean <- port49 %>%
  rename(date=x)

port49_clean$date <- parse_date(as.character(port49_clean$date), format = "%Y%m")

head(port49_clean) 
  
```


```{r}
ff <- read_csv("F-F_Research_Data_Factors.CSV", skip = 3, n_max = 1146, col_names = TRUE)
ff_clean <- ff %>%
  rename(date=X1)%>%
  clean_names()

ff_clean$date <- parse_date(as.character(ff_clean$date), format = "%Y%m")

head(ff_clean) 

```

# Question 3 - PCA for all datasets
```{r}
pca2<- function(x){
  ncol<- ncol(x)
  diffOmit <- diff(as.matrix(x[,2:ncol]))
  da <- na.omit(x)
  ddat <- na.omit(diffOmit)
  n <- dim(ddat)[1]
  options(digits = 4)
  pca<- prcomp(ddat,5)
  summary(pca)
}

pca_op<- pca2(op_clean)
print(pca_op)

pca_inv <- pca2(inv_clean)
print(pca_inv)

pca_dy <- pca2(dy_clean)
print(pca_dy)

pca_mom <- pca2(mom_clean)
print(pca_mom)

pca_port49 <- pca2(port49_clean)
print(pca_port49)
```


*Answer 3:* 
Principal component analysis aid to condense data to fewer dimensions in order to better understand and visualize the data. This is specially used when the data to be analysed is multiple dimensions and the goal of this is to help identify the principal components where the variation in data is maximal. 
The goal in our data set is to en-capture most of the information while reducing the dimensionality. Here, 95% of the return variation can be explained by:
 - 3 PC's for portfolios formed on operating profit and investment.
 - 4 PC's for portfolios formed on dividend yield. 
 - 3 PC's for portfolios formed on momentum.
 - 32 PC's for 49 industry portfolios.
In the above mentioned cases, we can just use the above-mentioned PC's to analyse the data and remove the rest. This action will reduce the high dimensionality of the data but still help us retain 95% of the information. As we can see the 49 industry portfolios require a lot more dimensions to explain 95% of the variation in returns which makes the visualisation as well as analysis a lot more complex. 
 

# Question 4 - Fama-french Regression 
```{r}
reg<- function(df){
  total <- inner_join(df,ff_clean, by="date")
  total$ex_ret1<- 0
  ncol<- ncol(total)
  adj_rsq <- {}
  for (i in 2:(ncol-5)){
    total[ncol]<- total[i] - total$rf #formula for excess returns
    linear<- lm(ex_ret1 ~ mkt_rf + smb+ hml, data=total)
    adj_rsq[i-1] <- summary(linear)$adj.r.squared
  }
  paste0(" ADJUSTED R-SQUARED FOR ", deparse(substitute(df)), " IS ", mean(adj_rsq), " MEDIAN IS ", median(adj_rsq), " S.D. IS ", sd(adj_rsq))
}

operating_profit2 <- reg(op_clean)
print(operating_profit2)

investment2<- reg(inv_clean)
print(investment2)

dividend_yeild2 <- reg(dy_clean)
print(dividend_yeild2)

momentum2 <- reg(mom_clean)
print(momentum2)

portfolio49_2<- reg(port49_clean)
print(portfolio49_2)


```

*Answer 4:*
This is the fama-french model that is trying to predict how dependent the excess returns of a portfolio (as compared to the risk-free returns) are on - market risk premium, SMB (small minus big), HML (high minus low). This is an extension of CAPM; that works better in most scenarios as it adds two extra elements of risk. Further, this model has an assumption that small companies perform better than big companies.

Here we can see that the most accuracy in prediction (R-squared used as a proxy) is for portfolios formed on operating profitability and the least accuracy or the 49 industry portfolios. Further, the 49 industry portfolios also have a very high standard deviation i.e. SMB, HML, and market risk premium just account for 52% of the variation in excess returns as well as have a very high standard deviation in the R-squared.


# Question 5 - Regression using PCA
```{r}
reg2 <- function(df) {
  total <- merge(df, ff_clean, by = "date")
  pca = prcomp(total[c(2:(ncol(total)-4))])
  adj_rsq <- c()
  for(i in 2:(ncol(total)-4)) {
    regdata <- total %>%
      summarise(
        ex_ret = (as.numeric(total[ , i]) - as.numeric(rf))
        )
    regdata <- cbind(regdata, pca$x[,1], pca$x[,2], pca$x[,3])
    lm_reg<- train(ex_ret~., data = regdata, method = "lm")
    adj_rsq <- c(adj_rsq, summary(lm_reg$finalModel)$adj.r.squared)
  }
  paste0(" Found Adjusted R-squared for ", deparse(substitute(df)), " is ", mean(adj_rsq), " median is ", median(adj_rsq), "S.D. is ", sd(adj_rsq))
}


operating_profit3 <- reg2(op_clean)
print(operating_profit3)

investment2<- reg2(inv_clean)
print(investment2)

dividend_yeild2 <- reg2(dy_clean)
print(dividend_yeild2)

momentum2 <- reg2(mom_clean)
print(momentum2)

portfolio49_2<- reg2(port49_clean)
print(portfolio49_2)

```

*Answer 5:*
As we can see, the Adjusted R-squared has improved for all the portfolios (higher mean, median, and lower standard deviation). Further, the R-squared for portfolios formed on operating profit is the highest and for 49 industry portfolios is the lowest. This is understandable as thee 3 PC's in portfolio's formed on investment only explain 63.3% of the data. 

When compared with the previous regression, we can say that the regression using PCA has preformed better than the fama-french model i.e. owing to the - higher R-squared mean, median, and lower standard deviation, the 3 PC's explain more variability in the excess portfolio returns as compared to the market risk premium, SMB, and HML used in the fama-french model. Therefore, we have successfully been able to reduce the dimensionality of data without loosing out much information. 

NOTE: This improvement in R-squared is specially a lot more for the portfolios formed on dividend yield and momentum.

*Answer 6:*
Although PCA is an efficient way of reducing the dimensionality of big data, it has some limitations (due to it's complexity in understanding) as well as ways to ovecome those limitations which are explained below: 
 - It doesn't tell us exactly which factors have how much impact as it tends to club features in order to reduce the dimensionality of the data. Therefore, as we can see in our case also, we know that the 3 PC's explain 95.4% of the variation in excess portfolio returns for portfolios formed on operating profit. But we don't know which variables impact this excess return in specific? how much is the impact of these variables? which variables are redundant?
 - In case of financial models, the prices/reeturns may be skewed or have non-linear relationships and PCA is not very good at capturing these relations. Thefore, we need to transform them into linear variables and reelationships before running a PCA.
 - PCA is very senitive to the scale of the inputs/features i.e. the higher the range of a variable, the more likely it is to be one of the earlier PC's irrespective of how much variability it actually explaind. Thefore, to assauge this problem we stnadrdise the data vefore running a PCA.
 - Lastly, there tends to be a presence of missing vales (data not available for a day) and outliers (as the data is skewewd, prices are volatile, hugely governed by public sentiment, and could be affected by extrenal happenings or black swan events) in economic and finnacial datasets.  PCA doesnt handle missing values or outliers well, so we need to remove outliers as well as missing values before doing a PCA.  


#Question 7 - Loading the dataframe
```{r}
PS4_Daily<- read_excel("PS4_Daily.xlsx", sheet=1, col_names = TRUE) %>%
  clean_names() 



```


# Question 8 - PCA 
```{r}

#remove NAs
PS4_Daily <- subset(PS4_Daily, PS4_Daily$sveny01 !="NA") 

PS4_Daily_numeric <- PS4_Daily %>% select(-date)

#convert to numeric for PCA
PS4_Daily_numeric <- as.data.frame(sapply(PS4_Daily_numeric, as.numeric))

# str(PS4_Daily_numeric) as we can see the data becomes numeric

PCA_yields<-prcomp(PS4_Daily_numeric, center = TRUE, scale. = TRUE)
# pca(PS4_Daily_numeric)
summary(PCA_yields)

PC1<-PCA_yields$x[,1]
PC2<-PCA_yields$x[,2]
PC3<-PCA_yields$x[,3]


final <- data.frame(PS4_Daily$date,PC1,PC2,PC3)

autoplot(PCA_yields)


final %>% ggplot() +
  geom_line(aes(x=PS4_Daily.date,y=PC1), color="green") +
  geom_line(aes(x=PS4_Daily.date,y=PC2), color="blue") +
  geom_line(aes(x=PS4_Daily.date,y=PC3), color="red") + 
  labs(
    x="Date",
    y="PCs",
    title = "Time series plot of PCs",
    subtitle="Green = PC1, Blue = PC2, Red = PC3")

```
*Answer 8:*
The first PC explains 98% of the variation in the data and the first 3 PCs explain 99% of the variation in the data. This is a very good trade-off between reducing the dimensionality without loosing out much information. Further, we also plotted a time series of the first 3 PCs.  We can see that PC1 has the most variation and PC2 and PC3 are relatively similar (but PC2 has more variation). 

# Question 9

```{r}
diff_10_1 <- PS4_Daily_numeric$sveny10 - PS4_Daily_numeric$sveny01

cor(PC1,PS4_Daily_numeric$sveny03) #-0.9846
# cor(PC1,PS4_Daily_numeric$sven20) # also very highly correlated

cor(PC2,diff_10_1) #-0.9447

```

*Answer 9:*
The correlation between the first principal component and the 3-year yield is -0.9846. PC1 and the three year yield are almost perfectly negatively correlated meaning that as the 3 year yield falls from 1980 to 2020, PC1 rises. Yields since 1980 to 2020 dropped significantly from around 10%+ (for all maturities) to around 1-2%. PC1 is capturing this variation/change in yields over time. It is also highly correlated to the other maturity yields.

The correlation between the second principal component and the difference between the 10-year and 1-year yield is -0.9447. The 2nd principal component has a very high correlation with the difference between the 10th and 1st year yields. The economic intuition for PC2 is that it is capturing the yield spread between the different maturities. Longer maturity bonds tend to have higher yields than shorter due to the duration risk. PC2 is capturing this difference in maturity yield spread between 10 and 1 year yields.


*Answer 10:*
For this part of the question, the necessary files were downloaded and the R code `Code Lecture5 plus.R` was run to gauge why equal weight portfolios were over performing as opposed to the portfolio's whose weights were found using historical data and maximum Sharpe ratios. 

We realized this is due to over fitting as the model doesn't perform well on the out of sample data. Therefore, to assuage this we changed the shrinkage parameters (reduces the effect of sampling variation) on LASSO and ridge so the model based on weights derived from Sharpe ratio's performs well. 



